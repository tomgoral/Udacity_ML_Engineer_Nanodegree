{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0_train_deploy_model\n",
    " \n",
    "by: Tom Goral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTLINE\n",
    "\n",
    "The purpose of this note book is to Train & Test Predictive Models using data prepared by 2.0_feature_engineering.ipynb, so that the most accurate model could be identified and deployed.\n",
    "\n",
    "1. [STEP 1: SETUP NOTEBOOK](#STEP-1:-SETUP-NOTEBOOK)\n",
    "2. [STEP 2: LOAD DATA](#STEP-2:-LOAD-DATA)\n",
    "3. [STEP 3: SPLIT INPUT INTO TRAIN, VALIDATE, TEST](#STEP-3:-SPLIT-INPUT-INTO-TRAIN,-VALIDATE,-TEST)\n",
    "4. [STEP 4: UPLOAD DATA TO S3](#STEP-4:-UPLOAD-DATA-TO-S3)\n",
    "5. [STEP 5: TRY SEVERAL PREDICTOR MODELS](#STEP-5:-TRY-SEVERAL-PREDICTOR-MODELS)\n",
    "6. [STEP 6: IDENTIFY THE BEST PREDICTOR](#STEP-6:-IDENTIFY-THE-BEST-PREDICTOR)\n",
    "7. [STEP 7: OPTIMIZE THE BEST PREDICTOR](#STEP-7:-OPTIMIZE-THE-BEST-PREDICTOR)\n",
    "8. [STEP 8: DEPLOY THE BEST PREDICTOR](#STEP-8:-DEPLOY-THE-BEST-PREDICTOR)\n",
    "9. [STEP 9 : TUNE BEST PREDICTOR](#STEP-9:-TUNE-BEST-PREDICTOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: SETUP NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM LIBRARIES\n",
    "from utilities.xl2df         import xl2df\n",
    "from utilities.hist_plot     import hist_plot\n",
    "from utilities.print_metrics import print_metrics\n",
    "from utilities.df2input      import df2input\n",
    "from utilities.dfNoHdr       import dfNoHdr\n",
    "\n",
    "\n",
    "\n",
    "# STANDARD LIBRARIES\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle, gzip, urllib.request, json\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = UserWarning, module = \"matplotlib\")# Suppress matplotlib user warnings\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')  # Display inline matplotlib plots with IPython\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from time import gmtime, strftime\n",
    "import datetime\n",
    "now = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session:  <sagemaker.session.Session object at 0x7fca65430128>\n",
      "   role:  arn:aws:iam::634491126024:role/service-role/AmazonSageMaker-ExecutionRole-20200619T082443\n",
      " bucket:  sagemaker-us-east-2-634491126024\n",
      " region:  us-east-2\n",
      " client:  <botocore.client.SageMaker object at 0x7fca38f68390>\n"
     ]
    }
   ],
   "source": [
    "# SAGEMAKER LIBRARIES\n",
    "\n",
    "import sagemaker\n",
    "from   sagemaker                         import get_execution_role\n",
    "from   sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from   sagemaker.predictor               import csv_serializer\n",
    "from   sagemaker.sklearn.processing      import SKLearnProcessor\n",
    "import boto3\n",
    "\n",
    "\n",
    "session  = sagemaker.Session()                # Identify SageMaker Session\n",
    "role     = get_execution_role()               # Identify IAM Role\n",
    "bucket   = session.default_bucket()           # Identify S3 bucket\n",
    "region   = boto3.Session().region_name        # Identify Region\n",
    "sm_boto3 = boto3.client('sagemaker')          # Identify Client\n",
    "\n",
    "print('session: ', session)\n",
    "print('   role: ', role)\n",
    "print(' bucket: ', bucket)\n",
    "print(' region: ', region)\n",
    "print(' client: ', sm_boto3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: LOAD DATA\n",
    "\n",
    "Load \"features & responses\" of the  cleaned, prepared data for training.<br>\n",
    "Load the original anonymized data to compare the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "reading file: data/features.xlsx , sheet: features, index_col: 0\n",
      "loaded File data/features.xlsx in 30 seconds\n",
      "rows: 7965, cols: 362, cells: 2883330\n",
      "\n",
      "\n",
      "reading file: data/response.xlsx , sheet: response, index_col: 0\n",
      "loaded File data/response.xlsx in 0 seconds\n",
      "rows: 7965, cols: 1, cells: 7965\n",
      "\n",
      "\n",
      "reading file: data/anonymous.xlsx , sheet: anonymous, index_col: 0\n",
      "loaded File data/anonymous.xlsx in 8 seconds\n",
      "rows: 55140, cols: 14, cells: 771960\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features =  xl2df('data/features.xlsx','features',0)  \n",
    "response =  xl2df('data/response.xlsx','response',0)\n",
    "df       =  xl2df('data/anonymous.xlsx','anonymous',0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: SPLIT INPUT INTO TRAIN, VALIDATE, TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "# estimators\n",
    "from sklearn.neighbors           import KNeighborsRegressor\n",
    "from sklearn.linear_model        import LinearRegression\n",
    "from sklearn.ensemble            import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.tree                import DecisionTreeRegressor\n",
    "from sklearn                     import svm, preprocessing\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "\n",
    "# model accuracy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   set has 3575 samples.\n",
      "Testing    set has 2629 samples.\n",
      "Validation set has 1761 samples.\n",
      "Total data set has 7965 samples.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random_state=42\n",
    "\n",
    "#  2/3 training and 1/3 testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, response, test_size=0.33, random_state=random_state)\n",
    "\n",
    "# 2/3 training and 1/3 validation sets.\n",
    "X_train, X_val, y_train, y_val   = train_test_split(X_train, y_train, test_size=0.33, random_state=random_state)\n",
    "\n",
    "#std_scaler  = StandardScaler()\n",
    "#X_train     = std_scaler.fit_transform(X_train)\n",
    "#X_val       = std_scaler.fit_transform(X_val)\n",
    "#X_test      = std_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "print (\"Training   set has {} samples.\".format(X_train.shape[0]))\n",
    "print (\"Testing    set has {} samples.\".format(X_test.shape[0]))\n",
    "print (\"Validation set has {} samples.\".format(X_val.shape[0]))\n",
    "print (\"Total data set has {} samples.\".format(X_train.shape[0]+X_val.shape[0]+X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: UPLOAD DATA TO S3\n",
    "\n",
    "When a training job is constructed using SageMaker, a container is executed which performs the training operation. This container is given access to data that is stored in S3. This means that we need to upload the data we want to use for training to S3. In addition, when we perform a batch transform job, SageMaker expects the input data to be stored on S3. We can use the SageMaker API to do this and hide some of the details.\n",
    "\n",
    "\n",
    "Training Data Formats\n",
    "Many Amazon SageMaker algorithms support training with data in CSV format. To use data in CSV format for training, in the input data channel specification, specify text/csv as the Content Type. Amazon SageMaker requires that a CSV file doesn't have a header record and that the target variable is in the first column. To run unsupervised learning algorithms that don't have a target, specify the number of label columns in the content type. For example, in this case 'text/csv;label_size=0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local data directory\n",
    "data_dir = 'data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE FILE WITH HEADER, INDEX, FEATURES,COL[0], TARGET\n",
    "\n",
    "FILE_TRAIN = os.path.join(data_dir, 'train.csv')\n",
    "FILE_VALID = os.path.join(data_dir, 'valid.csv')\n",
    "FILE_TEST  = os.path.join(data_dir, 'test.csv')\n",
    "\n",
    "# save to local directory\n",
    "pd.concat([y_train, X_train], axis=1).to_csv(FILE_TRAIN)\n",
    "pd.concat([y_val, X_val], axis=1).to_csv(FILE_VALID)\n",
    "pd.concat([y_test, X_test], axis=1).to_csv(FILE_TEST)\n",
    "\n",
    "# upload to S3 for SageMaker\n",
    "prefix   = 'capstone' \n",
    "s3_train = session.upload_data(FILE_TRAIN,  bucket=bucket, key_prefix=prefix)\n",
    "s3_valid = session.upload_data(FILE_VALID,  bucket=bucket, key_prefix=prefix)\n",
    "s3_test  = session.upload_data(FILE_TEST,   bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain = pd.read_csv(s3_train,index_col=0)\n",
    "dfTrain.head(1)\n",
    "dfTrainNH = dfNoHdr(dfTrain)\n",
    "dfTrainNH.head(1)\n",
    "dfValid = pd.read_csv(s3_valid,index_col=0)\n",
    "dfValid.head(1)\n",
    "dfValidNH = dfNoHdr(dfValid)\n",
    "dfValidNH.head(1)\n",
    "dfTest = pd.read_csv(s3_test,index_col=0)\n",
    "dfTest.head(1)\n",
    "dfTestNH = dfNoHdr(dfTest)\n",
    "dfTestNH.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: TRY SEVERAL  PREDICTOR MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This was by far the most difficult part of this capstone assignment !! <br>I absolutely struggled with application of SageMaker AWS containers , high and low level models , built-in and custom models :0( <br> Syntax and process was not straightforward to me either, nevertheless you will find an example of trying three approaches:<br>\n",
    "<br>\n",
    "1. SageMaker Custom Model with a script (Random Forest)<br>\n",
    "2. SageMaker Built-In Model (XgBoost)<br>\n",
    "3. Scikit Models run on my PC (Decision Tree, KNN, AdaBoost)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 SageMaker Custom Model with a script (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utilities/script.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile utilities/script.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "import subprocess as sb \n",
    "import sys\n",
    "mypackage = 'sagemaker'\n",
    "sb.call([sys.executable, \"-m\", \"pip\", \"install\", mypackage]) \n",
    "\n",
    "\n",
    "# estimators\n",
    "from sklearn.neighbors           import KNeighborsRegressor\n",
    "from sklearn.linear_model        import LinearRegression\n",
    "from sklearn.ensemble            import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.tree                import DecisionTreeRegressor\n",
    "from sklearn                     import svm, preprocessing\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "\n",
    "\n",
    "# model accuracy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # TODO instantiate a model from its artifact stored in model_dir\n",
    "    model =  joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return model\n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    \"\"\"Parse input data payload\n",
    "\n",
    "    We currently only take csv input. Since we need to process both labelled\n",
    "    and unlabelled data we first determine whether the label column is present\n",
    "    by looking at how many columns were provided.\n",
    "    \"\"\"\n",
    "    if content_type == 'text/csv':\n",
    "        # Read the raw input data as CSV.\n",
    "        df = pd.read_csv(StringIO(input_data), \n",
    "                         header=None)\n",
    "\n",
    "        if len(df.columns) == len(feature_columns_names) + 1:\n",
    "            # This is a labelled example, includes the ring label\n",
    "            df.columns = feature_columns_names + [label_column]\n",
    "        elif len(df.columns) == len(feature_columns_names):\n",
    "            # This is an unlabelled example.\n",
    "            df.columns = feature_columns_names\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(\"{} not supported by script!\".format(content_type))\n",
    "\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    \"\"\"Format prediction output\n",
    "\n",
    "    The default accept/content-type between containers for serial inference is JSON.\n",
    "    We also want to set the ContentType or mimetype as the same value as accept so the next\n",
    "    container can read the response payload correctly.\n",
    "    \"\"\"\n",
    "    if accept == \"application/json\":\n",
    "        instances = []\n",
    "        for row in prediction.tolist():\n",
    "            instances.append({\"features\": row})\n",
    "\n",
    "        json_output = {\"instances\": instances}\n",
    "\n",
    "        return worker.Response(json.dumps(json_output), accept, mimetype=accept)\n",
    "    elif accept == 'text/csv':\n",
    "        return worker.Response(encoders.encode(prediction, accept), accept, mimetype=accept)\n",
    "    else:\n",
    "        raise RuntimeException(\"{} accept type is not supported by this script.\".format(accept))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"Preprocess input data\n",
    "\n",
    "    We implement this because the default predict_fn uses .predict(), but our model is a preprocessor\n",
    "    so we want to use .transform().\n",
    "\n",
    "    The output is returned in the following order:\n",
    "\n",
    "        rest of features either one hot encoded or standardized\n",
    "    \"\"\"\n",
    "    features = model.transform(input_data)\n",
    "\n",
    "    if label_column in input_data:\n",
    "        # Return the label (as the first column) and the set of features.\n",
    "        return np.insert(features, 0, input_data[label_column], axis=1)\n",
    "    else:\n",
    "        # Return only the set of features\n",
    "        return features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\n",
    "    print('\\n1. EXTRACT ARGUMENTS:')\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--choice', type=str, default ='rfr')\n",
    "    parser.add_argument('--random_state', type=int, default=42)\n",
    "    parser.add_argument('--n-estimators', type=int, default=10)\n",
    "    parser.add_argument('--min-samples-leaf', type=int, default=3)\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--train-file', type=str, default='train.csv')\n",
    "    parser.add_argument('--test-file', type=str, default='test.csv')\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "   \n",
    "    print('2.         LOAD DATA:')\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file),index_col =0)\n",
    "    train_df = train_df.astype('float32')\n",
    "    X_train = train_df[train_df.columns[1:]]\n",
    "    y_train = train_df[train_df.columns[0]]    \n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file),index_col =0)\n",
    "    test_df = test_df.astype('float32')\n",
    "    X_test = test_df[test_df.columns[1:]]\n",
    "    y_test = test_df[test_df.columns[0]]\n",
    "    \n",
    "\n",
    "    print('3.        LOAD MODEL: ',args.choice)\n",
    "    \n",
    "    if args.choice == 'tree':        \n",
    "        model = DecisionTreeRegressor(random_state=args.random_state)  # baseline\n",
    "        \n",
    "    elif args.choice == 'knn':        \n",
    "        model =  KNeighborsRegressor()\n",
    "       \n",
    "    elif args.choice == 'rfr':        \n",
    "        model = RandomForestRegressor(\n",
    "                n_estimators=args.n_estimators,\n",
    "                min_samples_leaf=args.min_samples_leaf,\n",
    "                random_state = args.random_state,\n",
    "                n_jobs=-1)\n",
    "        \n",
    "    elif args.choice == 'ada':        \n",
    "        model =  AdaBoostRegressor(random_state=args.random_state)\n",
    "    \n",
    "    elif args.choice == 'linreg':\n",
    "        model =  LinearRegression()  \n",
    "    \n",
    "\n",
    "    print('4.         FIT MODEL: ',args.choice)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    print('5.        TEST MODEL: ',args.choice)\n",
    "    preds_model     = model.predict(X_test)\n",
    "    print('         mse: {:.4f}'.format(mean_squared_error(y_test,preds_model)))\n",
    "    print('        rmse: {:.4f}'.format(np.sqrt(mean_squared_error(y_test,preds_model))))\n",
    "    print('         mae: {:.4f}'.format(mean_absolute_error(y_test,preds_model)))\n",
    "    print('          r2: {:.4f}'.format(r2_score(y_test,preds_model)))\n",
    "     \n",
    "        \n",
    "    \n",
    "    path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    joblib.dump(model, path)\n",
    "    print('6.    SAVED MODEL TO: ' + path)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "from io import StringIO\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "csv_serializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (1.71.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==0.1.4 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker) (0.1.4)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker) (1.18.1)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker) (3.12.2)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker) (1.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker) (20.3)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker) (1.4.1)\n",
      "Requirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker) (1.14.27)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker) (46.1.3.post20200330)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (2.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker) (2.4.6)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.27 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker) (1.17.27)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.27->boto3>=1.14.12->sagemaker) (1.25.8)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.27->boto3>=1.14.12->sagemaker) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.27->boto3>=1.14.12->sagemaker) (2.8.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "1. EXTRACT ARGUMENTS:\n",
      "2.         LOAD DATA:\n",
      "3.        LOAD MODEL:  rfr\n",
      "4.         FIT MODEL:  rfr\n",
      "5.        TEST MODEL:  rfr\n",
      "         mse: 0.0034\n",
      "        rmse: 0.0579\n",
      "         mae: 0.0260\n",
      "          r2: 0.8360\n",
      "6.    SAVED MODEL TO: ./model.joblib\n"
     ]
    }
   ],
   "source": [
    "# Run from command line\n",
    "\n",
    "! python utilities/script.py --choice rfr\\\n",
    "                             --n-estimators 100 \\\n",
    "                             --min-samples-leaf 2 \\\n",
    "                             --model-dir ./ \\\n",
    "                             --train ./data \\\n",
    "                             --test ./data \\\n",
    "                             --train-file train.csv\\\n",
    "                             --test-file test.csv\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATE SageMaker Custom Model , SciKit Random Forest \n",
    "\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "estimator = SKLearn(\n",
    "    entry_point='utilities/script.py',\n",
    "    role = role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c5.xlarge',\n",
    "    framework_version='0.23-1',\n",
    "    base_job_name='tmg',\n",
    "    hyperparameters = {'n-estimators': 100,\n",
    "                       'min-samples-leaf': 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-02 23:02:50 Starting - Starting the training job...\n",
      "2020-08-02 23:02:52 Starting - Launching requested ML instances......\n",
      "2020-08-02 23:03:55 Starting - Preparing the instances for training...\n",
      "2020-08-02 23:04:34 Downloading - Downloading input data\n",
      "2020-08-02 23:04:34 Training - Downloading the training image.....\u001b[34m2020-08-02 23:05:29,923 sagemaker-training-toolkit INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2020-08-02 23:05:29,924 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-02 23:05:29,933 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-08-02 23:05:30,170 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-02 23:05:31,586 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-02 23:05:31,596 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-02 23:05:31,605 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"n-estimators\": 100,\n",
      "        \"min-samples-leaf\": 3\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tmg-2020-08-02-23-02-50-125\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-634491126024/tmg-2020-08-02-23-02-50-125/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"script\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"script.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"min-samples-leaf\":3,\"n-estimators\":100}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=script.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=script\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-634491126024/tmg-2020-08-02-23-02-50-125/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"min-samples-leaf\":3,\"n-estimators\":100},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tmg-2020-08-02-23-02-50-125\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-634491126024/tmg-2020-08-02-23-02-50-125/source/sourcedir.tar.gz\",\"module_name\":\"script\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"script.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--min-samples-leaf\",\"3\",\"--n-estimators\",\"100\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_N-ESTIMATORS=100\u001b[0m\n",
      "\u001b[34mSM_HP_MIN-SAMPLES-LEAF=3\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python script.py --min-samples-leaf 3 --n-estimators 100\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mCollecting sagemaker\n",
      "  Downloading sagemaker-1.72.0.tar.gz (297 kB)\u001b[0m\n",
      "\u001b[34mCollecting boto3>=1.14.12\n",
      "  Downloading boto3-1.14.33-py2.py3-none-any.whl (129 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.9.0 in /miniconda3/lib/python3.7/site-packages (from sagemaker) (1.18.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.1 in /miniconda3/lib/python3.7/site-packages (from sagemaker) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.19.0 in /miniconda3/lib/python3.7/site-packages (from sagemaker) (1.4.1)\u001b[0m\n",
      "\u001b[34mCollecting protobuf3-to-dict>=0.1.5\n",
      "  Downloading protobuf3-to-dict-0.1.5.tar.gz (3.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting smdebug-rulesconfig==0.1.4\n",
      "  Downloading smdebug_rulesconfig-0.1.4-py2.py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-metadata>=1.4.0\n",
      "  Downloading importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)\u001b[0m\n",
      "\u001b[34mCollecting packaging>=20.0\n",
      "  Downloading packaging-20.4-py2.py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /miniconda3/lib/python3.7/site-packages (from boto3>=1.14.12->sagemaker) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /miniconda3/lib/python3.7/site-packages (from boto3>=1.14.12->sagemaker) (0.3.3)\u001b[0m\n",
      "\u001b[34mCollecting botocore<1.18.0,>=1.17.33\n",
      "  Downloading botocore-1.17.33-py2.py3-none-any.whl (6.5 MB)\u001b[0m\n",
      "\n",
      "2020-08-02 23:05:47 Uploading - Uploading generated training model\n",
      "2020-08-02 23:05:47 Completed - Training job completed\n",
      "\u001b[34mRequirement already satisfied: setuptools in /miniconda3/lib/python3.7/site-packages (from protobuf>=3.1->sagemaker) (47.1.1.post20200604)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9 in /miniconda3/lib/python3.7/site-packages (from protobuf>=3.1->sagemaker) (1.15.0)\u001b[0m\n",
      "\u001b[34mCollecting zipp>=0.5\n",
      "  Downloading zipp-3.1.0-py3-none-any.whl (4.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyparsing>=2.0.2\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /miniconda3/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.33->boto3>=1.14.12->sagemaker) (1.25.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /miniconda3/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.33->boto3>=1.14.12->sagemaker) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils<0.16,>=0.10 in /miniconda3/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.33->boto3>=1.14.12->sagemaker) (0.15.2)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker, protobuf3-to-dict\n",
      "  Building wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sagemaker (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker: filename=sagemaker-1.72.0-py2.py3-none-any.whl size=386358 sha256=8f5b3a1147223587cd36cfa7b7dedf0d179c39fa22fbb209bd8d5ec6cdd0cbc0\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/be/f6/123e0568433d4bfc1cb662c3cd20784d9e723b3260524bb93e\n",
      "  Building wheel for protobuf3-to-dict (setup.py): started\n",
      "  Building wheel for protobuf3-to-dict (setup.py): finished with status 'done'\n",
      "  Created wheel for protobuf3-to-dict: filename=protobuf3_to_dict-0.1.5-py3-none-any.whl size=4029 sha256=4bb8d802ab08d7c6f724d6c9d5410e3bb11ec664fe5c27de005b99f2fa72e31d\n",
      "  Stored in directory: /root/.cache/pip/wheels/ce/a0/8b/995ce2fbaf0e9fe7eb91da84e99e84d1b35cfaa555f2b8f1c7\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker protobuf3-to-dict\u001b[0m\n",
      "\u001b[34mInstalling collected packages: botocore, boto3, protobuf3-to-dict, smdebug-rulesconfig, zipp, importlib-metadata, pyparsing, packaging, sagemaker\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.16.25\n",
      "    Uninstalling botocore-1.16.25:\n",
      "      Successfully uninstalled botocore-1.16.25\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.13.25\n",
      "    Uninstalling boto3-1.13.25:\n",
      "      Successfully uninstalled boto3-1.13.25\u001b[0m\n",
      "\u001b[34mSuccessfully installed boto3-1.14.33 botocore-1.17.33 importlib-metadata-1.7.0 packaging-20.4 protobuf3-to-dict-0.1.5 pyparsing-2.4.7 sagemaker-1.72.0 smdebug-rulesconfig-0.1.4 zipp-3.1.0\u001b[0m\n",
      "\u001b[34m1. EXTRACT ARGUMENTS:\u001b[0m\n",
      "\u001b[34m2.         LOAD DATA:\u001b[0m\n",
      "\u001b[34m3.        LOAD MODEL:  rfr\u001b[0m\n",
      "\u001b[34m4.         FIT MODEL:  rfr\u001b[0m\n",
      "\u001b[34m5.        TEST MODEL:  rfr\n",
      "         mse: 0.0035\n",
      "        rmse: 0.0595\n",
      "         mae: 0.0278\n",
      "          r2: 0.8270\u001b[0m\n",
      "\u001b[34m6.    SAVED MODEL TO: /opt/ml/model/model.joblib\u001b[0m\n",
      "\u001b[34m2020-08-02 23:05:39,866 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 80\n",
      "Billable seconds: 80\n"
     ]
    }
   ],
   "source": [
    "# TRAIN SageMaker Custom Model , SciKit Random Forest\n",
    "%time\n",
    "estimator.fit({'train':s3_train, 'test': s3_test}, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.15 µs\n",
      "-------------!tmg-2020-08-02-23-02-50-125\n"
     ]
    }
   ],
   "source": [
    "# DEPLOY SageMaker Custom Model , SciKit Random Forest\n",
    "\n",
    "%time\n",
    "predictor = estimator.deploy(instance_type='ml.c5.xlarge', initial_instance_count=1)\n",
    "print(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_estimator.latest_training_job.wait(logs='None')\n",
    "artifact = sm_boto3.describe_training_job(\n",
    "    TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "print('Model artifact persisted at ' + artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>500 Internal Server Error</title>\n<h1>Internal Server Error</h1>\n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>\n\". See https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logEventViewer:group=/aws/sagemaker/Endpoints/tmg-2020-08-02-23-02-50-125 in account 634491126024 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-97859c88813f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>500 Internal Server Error</title>\n<h1>Internal Server Error</h1>\n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>\n\". See https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logEventViewer:group=/aws/sagemaker/Endpoints/tmg-2020-08-02-23-02-50-125 in account 634491126024 for more information."
     ]
    }
   ],
   "source": [
    "predictor.predict(s3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SageMaker Custom Model , SciKit Random Forest\n",
    "\n",
    "%time\n",
    "print(predictor.predict(testX[data.feature_names]))\n",
    "y_preds = predictor.predict(X_test)\n",
    "print('mse: ',mean_squared_error(y_test,y_preds))\n",
    "print('mae: ',mean_absolute_error(y_test,y_preds))\n",
    "print('r2 : ',r2_score(y_test,y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE SageMaker Custom Model , SciKit Random Forest\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 SageMaker Built-In Model (XgBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATE SageMaker Built-In Model , xgboost \n",
    "\n",
    "container = get_image_uri(region, 'xgboost')\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container, # The image name of the training container\n",
    "                                    role,      # The IAM role to use (our current role in this case)\n",
    "                                    train_instance_count=1, # The number of instances to use for training\n",
    "                                    train_instance_type='ml.m4.xlarge', # The type of instance to use for training\n",
    "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                                                                        # Where to save the output (the model artifacts)\n",
    "                                    sagemaker_session=session) # The current SageMaker session\n",
    "\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        objective='reg:linear',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SageMaker Built-In Model , xgboost \n",
    "\n",
    "%time\n",
    "s3_input_train_xgb      = sagemaker.s3_input(s3_data=s3_train_xgb, content_type='csv')\n",
    "s3_input_validation_xgb = sagemaker.s3_input(s3_data=s3_validation_xgb, content_type='csv')\n",
    "\n",
    "xgb.fit({'train': s3_input_train_xgb , 'validation': s3_input_validation_xgb}, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SageMaker Built-In Model , xgboost\n",
    "\n",
    "xgb_transformer = xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')\n",
    "xgb_transformer.transform(s3_test, content_type='text/csv', split_type='Line')\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir\n",
    "y_pred = pd.read_csv(os.path.join(data_dir, 'test_xgb.csv.out'), header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Scikit Models run on my PC (Decision Tree, KNN, AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate models\n",
    "random_state=42\n",
    "\n",
    "tree       = DecisionTreeRegressor(random_state=random_state)   #baseline\n",
    "knn        = KNeighborsRegressor()\n",
    "rfr        = RandomForestRegressor(random_state=random_state)\n",
    "ada        = AdaBoostRegressor(random_state=random_state)\n",
    "linreg     = LinearRegression()\n",
    "algorithms = {'tree': tree,'knn':knn,'rfr': rfr,'ada': ada, 'linreg':linreg}\n",
    "\n",
    "# Fit models\n",
    "\n",
    "tree.fit(X_train, y_train.to_numpy().ravel())\n",
    "knn.fit(X_train, y_train.to_numpy().ravel())\n",
    "rfr.fit(X_train, y_train.to_numpy().ravel())\n",
    "ada.fit(X_train, y_train.to_numpy().ravel())\n",
    "linreg.fit(X_train, y_train.to_numpy().ravel())\n",
    "\n",
    "# Test models\n",
    "\n",
    "preds_tree   = tree.predict(X_test)\n",
    "preds_knn    = knn.predict(X_test) \n",
    "preds_rfr    = rfr.predict(X_test)\n",
    "preds_ada    = ada.predict(X_test)\n",
    "preds_linreg = linreg.predict(X_test)\n",
    "predictions  = {'tree': preds_tree,'knn':preds_knn,'rfr': preds_rfr,'ada': preds_ada, 'linreg':preds_linreg}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: IDENTIFY THE BEST PREDICTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMetrics = pd.DataFrame(index=['mse','rmse','mae','r2'],columns=['tree','knn','rfr','ada','linreg','xgboost'])\n",
    "\n",
    "for k,v in predictions.items():\n",
    "    dfMetrics[k].loc['mse'] =round(mean_squared_error(y_test,v),4)\n",
    "    dfMetrics[k].loc['rmse']=round(np.sqrt(mean_squared_error(y_test,v)),4)\n",
    "    dfMetrics[k].loc['mae'] =round(mean_absolute_error(y_test,v),4)\n",
    "    dfMetrics[k].loc['r2']  =round(r2_score(y_test,v),4)\n",
    "    \n",
    "dfMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe to compare the predictions\n",
    "\n",
    "first = True\n",
    "\n",
    "for k,v in predictions.items():\n",
    "    \n",
    "    if first == True:\n",
    "        t = np.reshape(v, (len(y_test),1))\n",
    "\n",
    "    else:\n",
    "        v = np.reshape(v, (len(y_test),1))\n",
    "        t = np.concatenate((t,v), axis=-1)\n",
    "       \n",
    "    \n",
    "    first = False\n",
    "    \n",
    "dfResults = pd.DataFrame(t,columns=algorithms,index=y_test.index)\n",
    "dfResults\n",
    "\n",
    "\n",
    "\n",
    "dfResults['ACTUAL'] =\"\"\n",
    "dfResults['class']  =\"\"\n",
    "dfResults['sub']    =\"\"\n",
    "dfResults['assy']   =\"\"\n",
    "dfResults['head']   =\"\"\n",
    "dfResults['drive']  =\"\"\n",
    "dfResults['thread'] =\"\"\n",
    "dfResults['nom']    =\"\"\n",
    "dfResults['point']  =\"\"\n",
    "dfResults['heat']   =\"\"\n",
    "dfResults['lock']   =\"\"\n",
    "dfResults['plate']  =\"\"\n",
    "dfResults['qty']    =\"\"\n",
    "dfResults['mm']     =\"\"\n",
    "\n",
    "\n",
    "for each in dfResults.index:\n",
    "    dfResults['ACTUAL'].loc[each] = df['cost'].loc[each]\n",
    "    dfResults['class'].loc[each]  = df['class'].loc[each]\n",
    "    dfResults['sub'].loc[each]    = df['sub'].loc[each]\n",
    "    dfResults['assy'].loc[each]   = df['assy'].loc[each]\n",
    "    dfResults['head'].loc[each]   = df['head'].loc[each]\n",
    "    dfResults['drive'].loc[each]  = df['drive'].loc[each]\n",
    "    dfResults['thread'].loc[each] = df['thread'].loc[each]\n",
    "    dfResults['nom'].loc[each]    = df['nom'].loc[each]\n",
    "    dfResults['point'].loc[each]  = df['point'].loc[each]\n",
    "    dfResults['heat'].loc[each]   = df['heat'].loc[each]\n",
    "    dfResults['lock'].loc[each]   = df['lock'].loc[each]\n",
    "    dfResults['plate'].loc[each]  = df['plate'].loc[each]\n",
    "    dfResults['qty'].loc[each]    = df['qty'].loc[each]\n",
    "    dfResults['mm'].loc[each]     = df['mm'].loc[each]\n",
    "\n",
    "    \n",
    "dfResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "    \n",
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].scatter(y_test, preds_tree)\n",
    "axs[0, 0].set_title('Axis [0, 0]')\n",
    "axs[0, 1].scatter(y_test, preds_knn, 'tab:orange')\n",
    "axs[0, 1].set_title('Axis [0, 1]')\n",
    "axs[1, 0].scatter(y_test, preds_rfr, 'tab:green')\n",
    "axs[1, 0].set_title('Axis [1, 0]')\n",
    "axs[1, 1].scatter(y_test, preds_ada, 'tab:red')\n",
    "axs[1, 1].set_title('Axis [1, 1]')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='x-label', ylabel='y-label')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7 : OPTIMIZE THE  BEST PREDICTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8 : DEPLOY THE  BEST PREDICTOR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
