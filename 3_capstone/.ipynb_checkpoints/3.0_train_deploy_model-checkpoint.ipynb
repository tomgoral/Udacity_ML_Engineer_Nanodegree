{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0_train_deploy_model\n",
    " \n",
    "by: Tom Goral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTLINE\n",
    "\n",
    "The purpose of this note book is to Train & Test Predictive Models using data prepared by 2.0_feature_engineering.ipynb, so that the most accurate model could be identified and deployed.\n",
    "\n",
    "1. [STEP 1: SETUP NOTEBOOK](#STEP-1:-SETUP-NOTEBOOK)\n",
    "2. [STEP 2: LOAD DATA](#STEP-2:-LOAD-DATA)\n",
    "3. [STEP 3: SPLIT INPUT INTO TRAIN, VALIDATE, TEST](#STEP-3:-SPLIT-INPUT-INTO-TRAIN,-VALIDATE,-TEST)\n",
    "4. [STEP 4: UPLOAD DATA TO S3](#STEP-4:-UPLOAD-DATA-TO-S3)\n",
    "5. [STEP 5: TRY SEVERAL PREDICTOR MODELS](#STEP-5:-TRY-SEVERAL-PREDICTOR-MODELS)\n",
    "6. [STEP 6: IDENTIFY THE BEST PREDICTOR](#STEP-6:-IDENTIFY-THE-BEST-PREDICTOR)\n",
    "7. [STEP 7: OPTIMIZE THE BEST PREDICTOR](#STEP-7:-OPTIMIZE-THE-BEST-PREDICTOR)\n",
    "8. [STEP 8: DEPLOY THE BEST PREDICTOR](#STEP-8:-DEPLOY-THE-BEST-PREDICTOR)\n",
    "9. [STEP 9 : TUNE BEST PREDICTOR](#STEP-9:-TUNE-BEST-PREDICTOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: SETUP NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM LIBRARIES\n",
    "from utilities.xl2df         import xl2df\n",
    "from utilities.hist_plot     import hist_plot\n",
    "from utilities.print_metrics import print_metrics\n",
    "from utilities.df2input      import df2input\n",
    "\n",
    "\n",
    "# STANDARD LIBRARIES\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle, gzip, urllib.request, json\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = UserWarning, module = \"matplotlib\")# Suppress matplotlib user warnings\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')  # Display inline matplotlib plots with IPython\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from time import gmtime, strftime\n",
    "import datetime\n",
    "now = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAGEMAKER LIBRARIES\n",
    "\n",
    "import sagemaker\n",
    "from   sagemaker                         import get_execution_role\n",
    "from   sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from   sagemaker.predictor               import csv_serializer\n",
    "from   sagemaker.sklearn.processing      import SKLearnProcessor\n",
    "import boto3\n",
    "\n",
    "\n",
    "session  = sagemaker.Session()                # Identify SageMaker Session\n",
    "role     = get_execution_role()               # Identify IAM Role\n",
    "bucket   = session.default_bucket()           # Identify S3 bucket\n",
    "region   = boto3.Session().region_name        # Identify Region\n",
    "sm_boto3 = boto3.client('sagemaker')          # Identify Client\n",
    "\n",
    "print('session: ', session)\n",
    "print('   role: ', role)\n",
    "print(' bucket: ', bucket)\n",
    "print(' region: ', region)\n",
    "print(' client: ', sm_boto3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: LOAD DATA\n",
    "\n",
    "Load \"features & responses\" of the  cleaned, prepared data for training.<br>\n",
    "Load the original anonymized data to compare the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features =  xl2df('data/features.xlsx','features',0)  \n",
    "response =  xl2df('data/response.xlsx','response',0)\n",
    "df       =  xl2df('data/anonymous.xlsx','anonymous',0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: SPLIT INPUT INTO TRAIN, VALIDATE, TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "# estimators\n",
    "from sklearn.neighbors           import KNeighborsRegressor\n",
    "from sklearn.linear_model        import LinearRegression\n",
    "from sklearn.ensemble            import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.tree                import DecisionTreeRegressor\n",
    "from sklearn                     import svm, preprocessing\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "\n",
    "# model accuracy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_state=42\n",
    "\n",
    "#  2/3 training and 1/3 testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, response, test_size=0.33, random_state=random_state)\n",
    "\n",
    "# 2/3 training and 1/3 validation sets.\n",
    "X_train, X_val, y_train, y_val   = train_test_split(X_train, y_train, test_size=0.33, random_state=random_state)\n",
    "\n",
    "#std_scaler  = StandardScaler()\n",
    "#X_train     = std_scaler.fit_transform(X_train)\n",
    "#X_val       = std_scaler.fit_transform(X_val)\n",
    "#X_test      = std_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "print (\"Training   set has {} samples.\".format(X_train.shape[0]))\n",
    "print (\"Testing    set has {} samples.\".format(X_test.shape[0]))\n",
    "print (\"Validation set has {} samples.\".format(X_val.shape[0]))\n",
    "print (\"Total data set has {} samples.\".format(X_train.shape[0]+X_val.shape[0]+X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: UPLOAD DATA TO S3\n",
    "\n",
    "When a training job is constructed using SageMaker, a container is executed which performs the training operation. This container is given access to data that is stored in S3. This means that we need to upload the data we want to use for training to S3. In addition, when we perform a batch transform job, SageMaker expects the input data to be stored on S3. We can use the SageMaker API to do this and hide some of the details.\n",
    "\n",
    "\n",
    "Training Data Formats\n",
    "Many Amazon SageMaker algorithms support training with data in CSV format. To use data in CSV format for training, in the input data channel specification, specify text/csv as the Content Type. Amazon SageMaker requires that a CSV file doesn't have a header record and that the target variable is in the first column. To run unsupervised learning algorithms that don't have a target, specify the number of label columns in the content type. For example, in this case 'text/csv;label_size=0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local data directory\n",
    "data_dir = 'data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit Learn Input \n",
    "\n",
    "FILE_TRAIN      = os.path.join(data_dir, 'train.csv')\n",
    "FILE_VALIDATION = os.path.join(data_dir, 'validation.csv')\n",
    "FILE_TEST       = os.path.join(data_dir, 'test.csv')\n",
    "\n",
    "# save to local directory\n",
    "pd.concat([y_train, X_train], axis=1).to_csv(FILE_TRAIN)\n",
    "pd.concat([y_val, X_val], axis=1).to_csv(FILE_VALIDATION)\n",
    "pd.concat([y_test, X_test], axis=1).to_csv(FILE_TEST)\n",
    "\n",
    "# upload to S3 for SageMaker\n",
    "prefix         = 'capstone' \n",
    "s3_train       = session.upload_data(FILE_TRAIN,       bucket=bucket, key_prefix=prefix)\n",
    "s3_validation  = session.upload_data(FILE_VALIDATION,  bucket=bucket, key_prefix=prefix)\n",
    "s3_test        = session.upload_data(FILE_TEST,        bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_TRAIN_XGB      = os.path.join(data_dir, 'train_xgb.csv')\n",
    "FILE_VALIDATION_XGB = os.path.join(data_dir, 'validation_xgb.csv')\n",
    "FILE_TEST_XGB       = os.path.join(data_dir, 'test_xgb.csv')\n",
    "\n",
    "pd.concat([y_train, X_train], axis=1).to_csv(FILE_TRAIN_XGB, header=False, index=False)\n",
    "pd.concat([y_val, X_val], axis=1).to_csv(FILE_VALIDATION_XGB, header=False, index=False)\n",
    "X_test.to_csv(FILE_TEST_XGB, header=False, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prefix             = 'capstone' \n",
    "s3_train_xgb       = session.upload_data(FILE_TRAIN_XGB,       bucket=bucket, key_prefix=prefix)\n",
    "s3_validation_xgb  = session.upload_data(FILE_VALIDATION_XGB,  bucket=bucket, key_prefix=prefix)\n",
    "s3_test_xgb        = session.upload_data(FILE_TEST_XGB,        bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def csv2sk(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df.rename(columns = {'Unnamed: 0':''}, inplace = True)\n",
    "    df.set_index(df.iloc[:, 0], inplace = True)\n",
    "    del df['']\n",
    "    features = df.iloc[0:,1:]\n",
    "    response = df.iloc[0:, 0]\n",
    "    #response = df.iloc[0:, :1]\n",
    "   \n",
    "    return features, response\n",
    "\n",
    "train_features, train_response           = csv2sk(FILE_TRAIN_SKL)\n",
    "validation_features, validation_response = csv2sk(FILE_VALIDATION_SKL)\n",
    "test_features, test_response             = csv2sk(FILE_TEST_SKL)\n",
    "\n",
    "train_response'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: TRY SEVERAL  PREDICTOR MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This was by far the most difficult part of this capstone assignment !!  I absolutely struggled with application of SageMaker AWS containers , high and low level models , built-in and custom models.  Syntax and process was not straightforward to me either, nevertheless you will find an example of trying three approaches:<br>\n",
    "<br>\n",
    "1. SageMaker Custom Model with a script (Random Forest)<br>\n",
    "2. SageMaker Built-In Model (XgBoost)<br>\n",
    "3. Scikit Models run on my PC (Decision Tree, KNN, AdaBoost)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 SageMaker Custom Model with a script (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%writefile utilities/script.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "import subprocess as sb \n",
    "import sys\n",
    "mypackage = 'sagemaker'\n",
    "sb.call([sys.executable, \"-m\", \"pip\", \"install\", mypackage]) \n",
    "\n",
    "\n",
    "# estimators\n",
    "from sklearn.neighbors           import KNeighborsRegressor\n",
    "from sklearn.linear_model        import LinearRegression\n",
    "from sklearn.ensemble            import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.tree                import DecisionTreeRegressor\n",
    "from sklearn                     import svm, preprocessing\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "\n",
    "\n",
    "# model accuracy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # TODO instantiate a model from its artifact stored in model_dir\n",
    "    model =  joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return model\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    # TODO apply model to the input_data, return result of interest\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\n",
    "    print('\\n1. EXTRACT ARGUMENTS:')\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--choice', type=str, default ='rfr')\n",
    "    parser.add_argument('--random_state', type=int, default=42)\n",
    "    parser.add_argument('--n-estimators', type=int, default=10)\n",
    "    parser.add_argument('--min-samples-leaf', type=int, default=3)\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--train-file', type=str, default='train.csv')\n",
    "    parser.add_argument('--test-file', type=str, default='test.csv')\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "   \n",
    "    print('2.         LOAD DATA:')\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file),index_col =0)\n",
    "    train_df = train_df.astype('float32')\n",
    "    X_train = train_df[train_df.columns[1:]]\n",
    "    y_train = train_df[train_df.columns[0]]    \n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file),index_col =0)\n",
    "    test_df = test_df.astype('float32')\n",
    "    X_test = test_df[test_df.columns[1:]]\n",
    "    y_test = test_df[test_df.columns[0]]\n",
    "    \n",
    "\n",
    "    print('3.        LOAD MODEL: ',args.choice)\n",
    "    \n",
    "    if args.choice == 'tree':        \n",
    "        model = DecisionTreeRegressor(random_state=args.random_state)  # baseline\n",
    "        \n",
    "    elif args.choice == 'knn':        \n",
    "        model =  KNeighborsRegressor()\n",
    "       \n",
    "    elif args.choice == 'rfr':        \n",
    "        model = RandomForestRegressor(\n",
    "                n_estimators=args.n_estimators,\n",
    "                min_samples_leaf=args.min_samples_leaf,\n",
    "                random_state = args.random_state,\n",
    "                n_jobs=-1)\n",
    "        \n",
    "    elif args.choice == 'ada':        \n",
    "        model =  AdaBoostRegressor(random_state=args.random_state)\n",
    "    \n",
    "    elif args.choice == 'linreg':\n",
    "        model =  LinearRegression()  \n",
    "    \n",
    "\n",
    "    print('4.         FIT MODEL: ',args.choice)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    print('5.        TEST MODEL: ',args.choice)\n",
    "    preds_model     = model.predict(X_test)\n",
    "    print('         mse: {:.4f}'.format(mean_squared_error(y_test,preds_model)))\n",
    "    print('        rmse: {:.4f}'.format(np.sqrt(mean_squared_error(y_test,preds_model))))\n",
    "    print('         mae: {:.4f}'.format(mean_absolute_error(y_test,preds_model)))\n",
    "    print('          r2: {:.4f}'.format(r2_score(y_test,preds_model)))\n",
    "     \n",
    "        \n",
    "    \n",
    "    path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    joblib.dump(model, path)\n",
    "    print('6.    SAVED MODEL TO: ' + path)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run from command line\n",
    "\n",
    "! python utilities/script.py --choice rfr\\\n",
    "                             --n-estimators 100 \\\n",
    "                             --min-samples-leaf 2 \\\n",
    "                             --model-dir ./ \\\n",
    "                             --train ./data \\\n",
    "                             --test ./data \\\n",
    "                             --train-file train.csv\\\n",
    "                             --test-file test.csv\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATE SageMaker Custom Model , SciKit Random Forest \n",
    "\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "estimator = SKLearn(\n",
    "    entry_point='utilities/script.py',\n",
    "    role = role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c5.xlarge',\n",
    "    framework_version='0.23-1',\n",
    "    base_job_name='tmg',\n",
    "    hyperparameters = {'n-estimators': 100,\n",
    "                       'min-samples-leaf': 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SageMaker Custom Model , SciKit Random Forest\n",
    "\n",
    "%time\n",
    "trainpath = s3_train\n",
    "testpath  = s3_test\n",
    "estimator.fit({'train':trainpath, 'test': testpath}, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPLOY SageMaker Custom Model , SciKit Random Forest\n",
    "\n",
    "%time\n",
    "predictor = estimator.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1)\n",
    "print(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SageMaker Custom Model , SciKit Random Forest\n",
    "\n",
    "%time\n",
    "y_preds = predictor.predict(X_test)\n",
    "print('mse: ',mean_squared_error(y_test,y_preds))\n",
    "print('mae: ',mean_absolute_error(y_test,y_preds))\n",
    "print('r2 : ',r2_score(y_test,y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE SageMaker Custom Model , SciKit Random Forest\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 SageMaker Built-In Model (XgBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATE SageMaker Built-In Model , xgboost \n",
    "\n",
    "container = get_image_uri(region, 'xgboost')\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container, # The image name of the training container\n",
    "                                    role,      # The IAM role to use (our current role in this case)\n",
    "                                    train_instance_count=1, # The number of instances to use for training\n",
    "                                    train_instance_type='ml.m4.xlarge', # The type of instance to use for training\n",
    "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                                                                        # Where to save the output (the model artifacts)\n",
    "                                    sagemaker_session=session) # The current SageMaker session\n",
    "\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        objective='reg:linear',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SageMaker Built-In Model , xgboost \n",
    "\n",
    "%time\n",
    "s3_input_train      = sagemaker.s3_input(s3_data=s3_train, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=s3_validation, content_type='csv')\n",
    "\n",
    "xgb.fit({'train': s3_input_train , 'validation': s3_input_validation}, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SageMaker Built-In Model , xgboost\n",
    "\n",
    "xgb_transformer = xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')\n",
    "xgb_transformer.transform(s3_test, content_type='text/csv', split_type='Line')\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir\n",
    "y_pred = pd.read_csv(os.path.join(data_dir, 'test_xgb.csv.out'), header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Scikit Models run on my PC (Decision Tree, KNN, AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate models\n",
    "random_state=42\n",
    "\n",
    "tree       = DecisionTreeRegressor(random_state=random_state)   #baseline\n",
    "knn        = KNeighborsRegressor()\n",
    "rfr        = RandomForestRegressor(random_state=random_state)\n",
    "ada        = AdaBoostRegressor(random_state=random_state)\n",
    "linreg     = LinearRegression()\n",
    "algorithms = {'tree': tree,'knn':knn,'rfr': rfr,'ada': ada, 'linreg':linreg}\n",
    "\n",
    "# Fit models\n",
    "\n",
    "tree.fit(X_train, y_train.to_numpy().ravel())\n",
    "knn.fit(X_train, y_train.to_numpy().ravel())\n",
    "rfr.fit(X_train, y_train.to_numpy().ravel())\n",
    "ada.fit(X_train, y_train.to_numpy().ravel())\n",
    "linreg.fit(X_train, y_train.to_numpy().ravel())\n",
    "\n",
    "# Test models\n",
    "\n",
    "preds_tree   = tree.predict(X_test)\n",
    "preds_knn    = knn.predict(X_test) \n",
    "preds_rfr    = rfr.predict(X_test)\n",
    "preds_ada    = ada.predict(X_test)\n",
    "preds_linreg = linreg.predict(X_test)\n",
    "predictions  = {'tree': preds_tree,'knn':preds_knn,'rfr': preds_rfr,'ada': preds_ada, 'linreg':preds_linreg}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: IDENTIFY THE BEST PREDICTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMetrics = pd.DataFrame(index=['mse','rmse','mae','r2'],columns=['tree','knn','rfr','ada','linreg','xgboost'])\n",
    "\n",
    "for k,v in predictions.items():\n",
    "    dfMetrics[k].loc['mse'] =round(mean_squared_error(y_test,v),4)\n",
    "    dfMetrics[k].loc['rmse']=round(np.sqrt(mean_squared_error(y_test,v)),4)\n",
    "    dfMetrics[k].loc['mae'] =round(mean_absolute_error(y_test,v),4)\n",
    "    dfMetrics[k].loc['r2']  =round(r2_score(y_test,v),4)\n",
    "    \n",
    "dfMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe to compare the predictions\n",
    "\n",
    "first = True\n",
    "\n",
    "for k,v in predictions.items():\n",
    "    \n",
    "    if first == True:\n",
    "        t = np.reshape(v, (len(y_test),1))\n",
    "\n",
    "    else:\n",
    "        v = np.reshape(v, (len(y_test),1))\n",
    "        t = np.concatenate((t,v), axis=-1)\n",
    "       \n",
    "    \n",
    "    first = False\n",
    "    \n",
    "dfResults = pd.DataFrame(t,columns=algorithms,index=y_test.index)\n",
    "dfResults\n",
    "\n",
    "\n",
    "\n",
    "dfResults['ACTUAL'] =\"\"\n",
    "dfResults['class']  =\"\"\n",
    "dfResults['sub']    =\"\"\n",
    "dfResults['assy']   =\"\"\n",
    "dfResults['head']   =\"\"\n",
    "dfResults['drive']  =\"\"\n",
    "dfResults['thread'] =\"\"\n",
    "dfResults['nom']    =\"\"\n",
    "dfResults['point']  =\"\"\n",
    "dfResults['heat']   =\"\"\n",
    "dfResults['lock']   =\"\"\n",
    "dfResults['plate']  =\"\"\n",
    "dfResults['qty']    =\"\"\n",
    "dfResults['mm']     =\"\"\n",
    "\n",
    "\n",
    "for each in dfResults.index:\n",
    "    dfResults['ACTUAL'].loc[each] = df['cost'].loc[each]\n",
    "    dfResults['class'].loc[each]  = df['class'].loc[each]\n",
    "    dfResults['sub'].loc[each]    = df['sub'].loc[each]\n",
    "    dfResults['assy'].loc[each]   = df['assy'].loc[each]\n",
    "    dfResults['head'].loc[each]   = df['head'].loc[each]\n",
    "    dfResults['drive'].loc[each]  = df['drive'].loc[each]\n",
    "    dfResults['thread'].loc[each] = df['thread'].loc[each]\n",
    "    dfResults['nom'].loc[each]    = df['nom'].loc[each]\n",
    "    dfResults['point'].loc[each]  = df['point'].loc[each]\n",
    "    dfResults['heat'].loc[each]   = df['heat'].loc[each]\n",
    "    dfResults['lock'].loc[each]   = df['lock'].loc[each]\n",
    "    dfResults['plate'].loc[each]  = df['plate'].loc[each]\n",
    "    dfResults['qty'].loc[each]    = df['qty'].loc[each]\n",
    "    dfResults['mm'].loc[each]     = df['mm'].loc[each]\n",
    "\n",
    "    \n",
    "dfResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "    \n",
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].scatter(y_test, preds_tree)\n",
    "axs[0, 0].set_title('Axis [0, 0]')\n",
    "axs[0, 1].scatter(y_test, preds_knn, 'tab:orange')\n",
    "axs[0, 1].set_title('Axis [0, 1]')\n",
    "axs[1, 0].scatter(y_test, preds_rfr, 'tab:green')\n",
    "axs[1, 0].set_title('Axis [1, 0]')\n",
    "axs[1, 1].scatter(y_test, preds_ada, 'tab:red')\n",
    "axs[1, 1].set_title('Axis [1, 1]')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='x-label', ylabel='y-label')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7 : OPTIMIZE THE  BEST PREDICTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8 : DEPLOY THE  BEST PREDICTOR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
