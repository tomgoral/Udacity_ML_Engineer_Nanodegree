{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0_train_test_models\n",
    " \n",
    "by: Tom Goral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTLINE\n",
    "\n",
    "The purpose of this note book is to Train, Validate & Test Predictive Models using data from 1.1_clean_prepare_data.ipynb\n",
    "\n",
    "1. [STEP 1: SETUP NOTEBOOK](#STEP-1:-SETUP-NOTEBOOK)\n",
    "2. [STEP 2: LOAD DATA](#STEP-2:-LOAD-DATA)\n",
    "3. [STEP 3: SPLIT INTO TRAIN, VALIDATE, TEST](#STEP-3:-SPLIT-INTO-TRAIN,-VALIDATE,-TEST)\n",
    "4. [STEP 4: UPLOAD DATA TO S3](#STEP-4:-UPLOAD-DATA-TO-S3)\n",
    "5. [STEP 5: EXPLORE CANDIDATE PREDICTORS](#STEP-5:-EXPLORE-CANDIDATE-PREDICTORS)\n",
    "6. [STEP 6: TRAIN , FIT PREDICTORS](#STEP-6:-TRAIN-FIT-PREDICTORS)\n",
    "7. [STEP 7: TEST PREDICTORS](#STEP-7:-TEST-PREDICTORS)\n",
    "8. [STEP 8: IDENTIFY BEST PREDICTOR](#STEP-8:-IDENTIFY-BEST-PREDICTOR)\n",
    "9. [STEP 9 : TUNE BEST PREDICTOR](#STEP-9:-TUNE-BEST-PREDICTOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: SETUP NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM LIBRARIES\n",
    "from utilities.xl2df         import xl2df\n",
    "from utilities.hist_plot     import hist_plot\n",
    "from utilities.print_metrics import print_metrics\n",
    "from utilities.df2input      import df2input\n",
    "\n",
    "\n",
    "# STANDARD LIBRARIES\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle, gzip, urllib.request, json\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = UserWarning, module = \"matplotlib\")# Suppress matplotlib user warnings\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')  # Display inline matplotlib plots with IPython\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from time import gmtime, strftime\n",
    "import datetime\n",
    "now = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session:  <sagemaker.session.Session object at 0x7f2254063f28>\n",
      "   role:  arn:aws:iam::634491126024:role/service-role/AmazonSageMaker-ExecutionRole-20200619T082443\n",
      " bucket:  sagemaker-us-east-2-634491126024\n",
      " region:  us-east-2\n"
     ]
    }
   ],
   "source": [
    "# SAGEMAKER LIBRARIES\n",
    "\n",
    "import sagemaker\n",
    "from   sagemaker                         import get_execution_role\n",
    "from   sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from   sagemaker.predictor               import csv_serializer\n",
    "from   sagemaker.sklearn.processing      import SKLearnProcessor\n",
    "import boto3\n",
    "\n",
    "\n",
    "session  = sagemaker.Session()                # Identify SageMaker Session\n",
    "role     = get_execution_role()               # Identify IAM Role\n",
    "bucket   = session.default_bucket()           # Identify S3 bucket\n",
    "region   = boto3.Session().region_name        # Identify Region\n",
    "sm_boto3 = boto3.client('sagemaker')          # Identify Client\n",
    "\n",
    "print('session: ', session)\n",
    "print('   role: ', role)\n",
    "print(' bucket: ', bucket)\n",
    "print(' region: ',region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: LOAD DATA\n",
    "\n",
    "Load the cleaned , prepared data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "reading file: data/features.xlsx , sheet: features, index_col: 0\n",
      "loaded File data/features.xlsx in 31 seconds\n",
      "rows: 7965, cols: 362, cells: 2883330\n",
      "\n",
      "\n",
      "reading file: data/response.xlsx , sheet: response, index_col: 0\n",
      "loaded File data/response.xlsx in 0 seconds\n",
      "rows: 7965, cols: 1, cells: 7965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features =  xl2df('data/features.xlsx','features',0)  \n",
    "response =  xl2df('data/response.xlsx','response',0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: SPLIT INTO TRAIN, VALIDATE, TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training   set has 3575 samples.\n",
      "Validation set has 1761 samples.\n",
      "Testing    set has 2629 samples.\n",
      "Total data set has 7965 samples.\n"
     ]
    }
   ],
   "source": [
    "#  preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "# estimators\n",
    "from sklearn.neighbors    import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble     import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.tree         import DecisionTreeRegressor\n",
    "from sklearn              import svm, preprocessing\n",
    "\n",
    "\n",
    "# model accuracy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "random_state=42\n",
    "\n",
    "#  2/3 training and 1/3 testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, response, test_size=0.33, random_state=random_state)\n",
    "\n",
    "# 2/3 training and 1/3 validation sets.\n",
    "X_train, X_val, y_train, y_val   = train_test_split(X_train, y_train, test_size=0.33, random_state=random_state)\n",
    "\n",
    "#std_scaler  = StandardScaler()\n",
    "#X_train     = std_scaler.fit_transform(X_train)\n",
    "#X_val       = std_scaler.fit_transform(X_val)\n",
    "#X_test      = std_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "print (\"Training   set has {} samples.\".format(X_train.shape[0]))\n",
    "print (\"Validation set has {} samples.\".format(X_val.shape[0]))\n",
    "print (\"Testing    set has {} samples.\".format(X_test.shape[0]))\n",
    "print (\"Total data set has {} samples.\".format(X_train.shape[0]+X_val.shape[0]+X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: UPLOAD DATA TO S3\n",
    "\n",
    "When a training job is constructed using SageMaker, a container is executed which performs the training operation. This container is given access to data that is stored in S3. This means that we need to upload the data we want to use for training to S3. In addition, when we perform a batch transform job, SageMaker expects the input data to be stored on S3. We can use the SageMaker API to do this and hide some of the details.\n",
    "\n",
    "\n",
    "Training Data Formats\n",
    "Many Amazon SageMaker algorithms support training with data in CSV format. To use data in CSV format for training, in the input data channel specification, specify text/csv as the Content Type. Amazon SageMaker requires that a CSV file doesn't have a header record and that the target variable is in the first column. To run unsupervised learning algorithms that don't have a target, specify the number of label columns in the content type. For example, in this case 'text/csv;label_size=0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local data directory\n",
    "data_dir = 'data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit Learn Input \n",
    "\n",
    "FILE_TRAIN_SKL      = os.path.join(data_dir, 'train_skl.csv')\n",
    "FILE_VALIDATION_SKL = os.path.join(data_dir, 'validation_skl.csv')\n",
    "FILE_TEST_SKL       = os.path.join(data_dir, 'test_skl.csv')\n",
    "\n",
    "# save to local directory\n",
    "pd.concat([y_train, X_train], axis=1).to_csv(FILE_TRAIN_SKL)\n",
    "pd.concat([y_val, X_val], axis=1).to_csv(FILE_VALIDATION_SKL)\n",
    "pd.concat([y_test, X_test], axis=1).to_csv(FILE_TEST_SKL)\n",
    "\n",
    "# upload to S3 for SageMaker\n",
    "prefix         = 'capstone' \n",
    "s3_train_skl       = session.upload_data(FILE_TRAIN_SKL,       bucket=bucket, key_prefix=prefix)\n",
    "s3_validation_skl  = session.upload_data(FILE_VALIDATION_SKL,  bucket=bucket, key_prefix=prefix)\n",
    "s3_test_skl        = session.upload_data(FILE_TEST_SKL,        bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-97adfbeffa9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SM_CHANNEL_TRAIN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.environ.get('SM_CHANNEL_TRAIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_TRAIN_XGB      = os.path.join(data_dir, 'train_xgb.csv')\n",
    "FILE_VALIDATION_XGB = os.path.join(data_dir, 'validation_xgb.csv')\n",
    "FILE_TEST_XGB       = os.path.join(data_dir, 'test_xgb.csv')\n",
    "\n",
    "pd.concat([y_train, X_train], axis=1).to_csv(FILE_TRAIN_XGB, header=False, index=False)\n",
    "pd.concat([y_val, X_val], axis=1).to_csv(FILE_VALIDATION_XGB, header=False, index=False)\n",
    "X_test.to_csv(FILE_TEST_XGB, header=False, index=False)\n",
    "\n",
    "# upload to S3\n",
    "prefix         = 'capstone' \n",
    "s3_train       = session.upload_data(FILE_TRAIN_XGB,       bucket=bucket, key_prefix=prefix)\n",
    "s3_validation  = session.upload_data(FILE_VALIDATION_XGB,  bucket=bucket, key_prefix=prefix)\n",
    "s3_test        = session.upload_data(FILE_TEST_XGB,        bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv2sk(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df.rename(columns = {'Unnamed: 0':''}, inplace = True)\n",
    "    df.set_index(df.iloc[:, 0], inplace = True)\n",
    "    del df['']\n",
    "    features = df.iloc[0:,1:]\n",
    "    response = df.iloc[0:, 0]\n",
    "    #response = df.iloc[0:, :1]\n",
    "   \n",
    "    return features, response\n",
    "\n",
    "train_features, train_response           = csv2sk(FILE_TRAIN_SKL)\n",
    "validation_features, validation_response = csv2sk(FILE_VALIDATION_SKL)\n",
    "test_features, test_response             = csv2sk(FILE_TEST_SKL)\n",
    "\n",
    "train_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: EXPLORE CANDIDATE PREDICTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python script.py --n-estimators 20 \\\n",
    "                   --min-samples-leaf 2 \\\n",
    "                   --model-dir  $bucket \\\n",
    "                   --train-file $s3_train\\\n",
    "                   --test-file  $s3_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Estimator from the SageMaker Python SDK\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "sklearn_estimator = SKLearn( entry_point='script.py', role = role,\n",
    "                            train_instance_count=1, train_instance_type='ml.c5.xlarge',\n",
    "                             framework_version='0.23-1', base_job_name='xx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sklearn_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch training job, with asynchronous call\n",
    "\n",
    "s3_input_train      = sagemaker.s3_input(s3_data=s3_train, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=s3_validation, content_type='csv')\n",
    "sklearn_estimator.fit({'train':s3_train, 'val': s3_validation}, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
    "                           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "                           max_samples=None, min_impurity_decrease=0.0,\n",
    "                           min_impurity_split=None, min_samples_leaf=1,\n",
    "                           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                           n_estimators=100, n_jobs=None, oob_score=False,\n",
    "                           random_state=42, verbose=0, warm_start=False)\n",
    "\n",
    "\n",
    "random_grid = {'bootstrap': [True, False], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "              'max_features': ['auto', 'sqrt'],'min_samples_leaf': [1, 2, 4],'min_samples_split': [2, 5, 10],\n",
    "              'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
    "\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3,\n",
    "                               verbose=2, random_state=random_state)# Fit the random search model\n",
    "\n",
    "\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train      = sagemaker.s3_input(s3_data=s3_train, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=s3_validation, content_type='csv')\n",
    "xgb.fit{'train':s3_input_train, 'validaton': s3_input_validation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run directly from Notebook instead of S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree           = DecisionTreeRegressor(random_state=random_state)  # benchmark\n",
    "tree.fit(X_train, y_train)\n",
    "print(tree)\n",
    "print()\n",
    "preds_tree     = tree.predict(X_test)\n",
    "print('mse: ',mean_squared_error(y_test,preds_tree))\n",
    "print('mae: ',mean_absolute_error(y_test,preds_tree))\n",
    "print('r2 : ',r2_score(y_test,preds_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree           = DecisionTreeRegressor(random_state=random_state)  # benchmark\n",
    "tree.fit(train_features, train_response)\n",
    "print(tree)\n",
    "print()\n",
    "preds_tree     = tree.predict(test_features)\n",
    "print('mse: ',mean_squared_error(test_response,preds_tree))\n",
    "print('mae: ',mean_absolute_error(test_response,preds_tree))\n",
    "print('r2 : ',r2_score(test_response,preds_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn            = KNeighborsRegressor()\n",
    "knn.fit(X_train, y_train)\n",
    "print(knn)\n",
    "print()\n",
    "preds_knn     = knn.predict(X_test)\n",
    "print('mse: ',mean_squared_error(y_test,preds_knn))\n",
    "print('mae: ',mean_absolute_error(y_test,preds_knn))\n",
    "print('r2 : ',r2_score(y_test,preds_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn            = KNeighborsRegressor()\n",
    "knn.fit(train_features, train_response)\n",
    "print(knn)\n",
    "print()\n",
    "preds_knn     = knn.predict(test_features)\n",
    "print('mse: ',mean_squared_error(test_response,preds_knn))\n",
    "print('mae: ',mean_absolute_error(test_response,preds_knn))\n",
    "print('r2 : ',r2_score(test_response,preds_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr            = RandomForestRegressor(random_state=random_state)\n",
    "rfr.fit(X_train, y_train)\n",
    "print(rfr)\n",
    "print()\n",
    "preds_rfr     = rfr.predict(X_test)\n",
    "print('mse: ',mean_squared_error(y_test,preds_rfr))\n",
    "print('mae: ',mean_absolute_error(y_test,preds_rfr))\n",
    "print('r2 : ',r2_score(y_test,preds_rfr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr            = RandomForestRegressor(random_state=random_state)\n",
    "rfr.fit(train_features, train_response)\n",
    "print(rfr)\n",
    "print()\n",
    "preds_rfr     = rfr.predict(test_features)\n",
    "print('mse: ',mean_squared_error(test_response,preds_rfr))\n",
    "print('mae: ',mean_absolute_error(test_response,preds_rfr))\n",
    "print('r2 : ',r2_score(test_response,preds_rfr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada            = AdaBoostRegressor(random_state=random_state)\n",
    "ada.fit(X_train, y_train)\n",
    "print(ada)\n",
    "print()\n",
    "preds_ada     = ada.predict(X_test)\n",
    "print('mse: ',mean_squared_error(y_test,preds_ada))\n",
    "print('mae: ',mean_absolute_error(y_test,preds_ada))\n",
    "print('r2 : ',r2_score(y_test,preds_ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg         = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "print(linreg)\n",
    "print()\n",
    "preds_linreg     = linreg.predict(X_test)\n",
    "print('mse: ',mean_squared_error(y_test,preds_linreg))\n",
    "print('mae: ',mean_absolute_error(y_test,preds_linreg))\n",
    "print('r2 : ',r2_score(y_test,preds_linreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your import and estimator code, here\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "# instantiate a sklearn estimator\n",
    "estimator = SKLearn(entry_point='train.py',\n",
    "                    source_dir='utilities',\n",
    "                    role=role,\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.c4.xlarge',\n",
    "                    sagemaker_session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tree           = DecisionTreeRegressor(random_state=random_state)  # benchmark\n",
    "knn            = KNeighborsRegressor()\n",
    "rfr            = RandomForestRegressor(random_state=random_state)\n",
    "ada            = AdaBoostRegressor(random_state=random_state)\n",
    "linreg         = LinearRegression()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "container = get_image_uri(region, 'xgboost')                   # training container\n",
    "xgb = sagemaker.estimator.Estimator(container,                 # estimator object from container\n",
    "                                    role,     \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge', \n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=session)\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        objective='reg:linear',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=200)\n",
    "                                                                   \n",
    "s3_input_train      = sagemaker.s3_input(s3_data=s3_train, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=s3_validation, content_type='csv')\n",
    "\n",
    "\n",
    "\n",
    "algorithms          = {'tree': tree,'knn':knn,'rfr': rfr,'ada': ada, 'linreg':linreg, 'xgb':xgb}\n",
    "algorithm_list      = []\n",
    "\n",
    "for k,v in algorithms.items():\n",
    "    algorithm_list.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: TRAIN FIT PREDICTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN REGRESSION MODELS\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "knn.fit(X_train, y_train)\n",
    "rfr.fit(X_train, y_train)\n",
    "ada.fit(X_train, y_train)\n",
    "linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN REGRESSION MODELS\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "knn.fit(X_train, y_train)\n",
    "rfr.fit(X_train, y_train)\n",
    "ada.fit(X_train, y_train)\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})\n",
    "xgb_transformer = xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')\n",
    "xgb_transformer.transform(s3_test, content_type='text/csv', split_type='Line')\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: TEST PREDICTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_tree     = tree.predict(X_test)\n",
    "preds_knn      = knn.predict(X_test) \n",
    "preds_rfr      = rfr.predict(X_test)\n",
    "preds_ada      = ada.predict(X_test)\n",
    "preds_linreg   = linreg.predict(X_test)\n",
    "predictions    = {'tree': preds_tree,'knn':preds_knn,'rfr': preds_rfr,\n",
    "                  'ada': preds_ada, 'linreg':preds_linreg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "\n",
    "# We need to tell the endpoint what format the data we are sending is in\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "\n",
    "Y_pred = xgb_predictor.predict(X_test.values).decode('utf-8')\n",
    "# predictions is currently a comma delimited string and so we would like to break it up\n",
    "# as a numpy array.\n",
    "Y_pred = np.fromstring(Y_pred, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: IDENTIFY BEST PREDICTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  GOOD MODEL R2 ~ 1,   BAD MODEL R2 ~ 0\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "print('Decision Tree')\n",
    "print(mean_squared_error(y_test,preds_tree))\n",
    "print(mean_absolute_error(y_test,preds_tree))\n",
    "print('\\nKNN')\n",
    "print(mean_squared_error(y_test,preds_knn))\n",
    "print(mean_absolute_error(y_test,preds_knn))\n",
    "print('\\nRandom Forest Regressor')\n",
    "print(mean_squared_error(y_test,preds_rfr))\n",
    "print(mean_absolute_error(y_test,preds_rfr))\n",
    "print('\\nAdaBoost')\n",
    "print(mean_squared_error(y_test,preds_ada))\n",
    "print(mean_absolute_error(y_test,preds_ada))\n",
    "print('\\nLinear Regression')\n",
    "print(mean_squared_error(y_test,preds_linreg))\n",
    "print(mean_absolute_error(y_test,preds_linreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfResults=y_test\n",
    "\n",
    "first = True\n",
    "for k,v in predictions.items():\n",
    "    \n",
    "    if first == True:\n",
    "        t = np.reshape(v, (2629,1))\n",
    "    else:\n",
    "        v = np.reshape(v, (2629,1))\n",
    "        t = np.concatenate((t,v), axis=-1)\n",
    "    \n",
    "    first = False\n",
    "    \n",
    "dfPredictions = pd.DataFrame(t,columns=algorithm_list,index=y_test.index)\n",
    "\n",
    "for each in dfPredictions.columns:\n",
    "    dfResults[each] = dfPredictions[each]\n",
    "\n",
    "    \n",
    "dfResults.rename(columns={\"cost\": \"ACTUAL\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path\n",
    "'s3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9 : TUNE BEST PREDICTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  GO TO:  3.0_deploy_monitor.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
